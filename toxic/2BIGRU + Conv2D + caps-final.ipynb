{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "from keras.layers import K, Activation, Average, Maximum\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,GlobalMaxPooling2D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TerminateOnNaN\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir=\"data/\"\n",
    "train = pd.read_csv(data_dir+\"train.csv\")\n",
    "test = pd.read_csv(data_dir+\"test.csv\")\n",
    "submission = pd.read_csv(data_dir+\"sample_submission.csv\")\n",
    "\n",
    "embedding_path = data_dir+\"fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "#embedding_path = data_dir+\"glove840b300dtxt/glove.840B.300d.txt\"\n",
    "\n",
    "max_features = 30000\n",
    "max_len = 100\n",
    "embed_size = 300\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "train[\"comment_text\"].fillna(\"no comment\")\n",
    "test[\"comment_text\"].fillna(\"no comment\")\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(train, y, test_size = 0.1)\n",
    "\n",
    "raw_text_train = X_train[\"comment_text\"].str.lower()\n",
    "raw_text_valid = X_valid[\"comment_text\"].str.lower()\n",
    "raw_text_test = test[\"comment_text\"].str.lower()\n",
    "\n",
    "tk = Tokenizer(num_words = max_features, lower = True)\n",
    "tk.fit_on_texts(raw_text_train)\n",
    "X_train[\"comment_seq\"] = tk.texts_to_sequences(raw_text_train)\n",
    "X_valid[\"comment_seq\"] = tk.texts_to_sequences(raw_text_valid)\n",
    "test[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test)\n",
    "\n",
    "X_train = pad_sequences(X_train.comment_seq, maxlen = max_len)\n",
    "X_valid = pad_sequences(X_valid.comment_seq, maxlen = max_len)\n",
    "test = pad_sequences(test.comment_seq, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9055de15dd59ed4b0c5f5da3922775121cb080aa",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "cc55955b4eb1c36f7b4338a6d75215158280d1ec",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\nROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "            for i in range(len(list_classes)):\n",
    "                score2 = roc_auc_score(self.y_val[:,i], y_pred[:,i])\n",
    "                print(\"ROC-AUC of class {}- epoch: {:d} - score: {:.6f}\".format(list_classes[i],epoch+1, score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "ec39caf3396c6ea2da959c355ee62a0c177a34d9",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = \"best_model.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,save_best_only = True, mode = \"min\")\n",
    "ra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "lr_scheduler = LearningRateScheduler(schedule=lambda epoch_n: self.init_lr / (5**(epoch_n)), verbose = 1)\n",
    "TON = TerminateOnNaN()\n",
    "\n",
    "Routings = 5\n",
    "Num_capsule = 8\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 32\n",
    "\n",
    "def build_model(lr = 0.0):\n",
    "    \n",
    "    # Input\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    \n",
    "    # CNN2D\n",
    "    y = SpatialDropout1D(0.4)(x)\n",
    "    y = Reshape((max_len, embed_size, 1))(y)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    maxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1))(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1))(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1))(conv_2)\n",
    "    maxpool_3 = MaxPool2D(pool_size=(max_len - filter_sizes[3] + 1, 1))(conv_3)\n",
    "    y = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.1)(y)\n",
    "    \n",
    "    # Bigru conv1D\n",
    "    z = Bidirectional(GRU(128, activation='relu', return_sequences=True))(x)\n",
    "    conv1D = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(z)\n",
    "    avg_pool = GlobalAveragePooling1D()(conv1D)\n",
    "    max_pool = GlobalMaxPooling1D()(conv1D)\n",
    "    z = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    # Capsule\n",
    "    c = Bidirectional(GRU(128, activation='relu', dropout=dropout_p,recurrent_dropout=dropout_p, return_sequences=True))(x)\n",
    "    c = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True)(c)\n",
    "    c = Flatten()(c)\n",
    "    c = Dropout(dropout_p)(c)\n",
    "    \n",
    "    # output\n",
    "    a = Average()([y,z,c])\n",
    "    m = Maximum()([y,z,c])\n",
    "    out = concatenate([a,m])\n",
    "    out = Dropout(0.1)(out)\n",
    "    out = Dense(6, activation = \"sigmoid\")(out)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    # model\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, Y_train, batch_size = 32, epochs = 6, validation_data = (X_valid, Y_valid), \n",
    "                        verbose = 1, callbacks = [ra_val, check_point, early_stop, TON])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9809\n",
      "ROC-AUC - epoch: 1 - score: 0.987818\n",
      "ROC-AUC of class toxic- epoch: 1 - score: 0.981749\n",
      "ROC-AUC of class severe_toxic- epoch: 1 - score: 0.988598\n",
      "ROC-AUC of class obscene- epoch: 1 - score: 0.989557\n",
      "ROC-AUC of class threat- epoch: 1 - score: 0.995471\n",
      "ROC-AUC of class insult- epoch: 1 - score: 0.986915\n",
      "ROC-AUC of class identity_hate- epoch: 1 - score: 0.984617\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04490, saving model to best_model.hdf5\n",
      "143613/143613 [==============================] - 2224s 15ms/step - loss: 0.0525 - acc: 0.9809 - val_loss: 0.0449 - val_acc: 0.9827\n",
      "Epoch 2/4\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9839\n",
      "ROC-AUC - epoch: 2 - score: 0.988339\n",
      "ROC-AUC of class toxic- epoch: 2 - score: 0.982269\n",
      "ROC-AUC of class severe_toxic- epoch: 2 - score: 0.988708\n",
      "ROC-AUC of class obscene- epoch: 2 - score: 0.989866\n",
      "ROC-AUC of class threat- epoch: 2 - score: 0.995933\n",
      "ROC-AUC of class insult- epoch: 2 - score: 0.987237\n",
      "ROC-AUC of class identity_hate- epoch: 2 - score: 0.986024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04490 to 0.04235, saving model to best_model.hdf5\n",
      "143613/143613 [==============================] - 2155s 15ms/step - loss: 0.0425 - acc: 0.9839 - val_loss: 0.0424 - val_acc: 0.9834\n",
      "Epoch 3/4\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9850\n",
      "ROC-AUC - epoch: 3 - score: 0.987523\n",
      "ROC-AUC of class toxic- epoch: 3 - score: 0.982493\n",
      "ROC-AUC of class severe_toxic- epoch: 3 - score: 0.988691\n",
      "ROC-AUC of class obscene- epoch: 3 - score: 0.989059\n",
      "ROC-AUC of class threat- epoch: 3 - score: 0.995101\n",
      "ROC-AUC of class insult- epoch: 3 - score: 0.986801\n",
      "ROC-AUC of class identity_hate- epoch: 3 - score: 0.982995\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "143613/143613 [==============================] - 2236s 16ms/step - loss: 0.0391 - acc: 0.9850 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "Epoch 4/4\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9861\n",
      "ROC-AUC - epoch: 4 - score: 0.985446\n",
      "ROC-AUC of class toxic- epoch: 4 - score: 0.981203\n",
      "ROC-AUC of class severe_toxic- epoch: 4 - score: 0.988149\n",
      "ROC-AUC of class obscene- epoch: 4 - score: 0.988044\n",
      "ROC-AUC of class threat- epoch: 4 - score: 0.988737\n",
      "ROC-AUC of class insult- epoch: 4 - score: 0.986671\n",
      "ROC-AUC of class identity_hate- epoch: 4 - score: 0.979872\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "143613/143613 [==============================] - 2422s 17ms/step - loss: 0.0355 - acc: 0.9861 - val_loss: 0.0446 - val_acc: 0.9829\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: Capsule",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b0bd49c3fcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-96dfcd095de1>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(lr)\u001b[0m\n\u001b[1;32m     61\u001b[0m     history = model.fit(X_train, Y_train, batch_size = 32, epochs = 4, validation_data = (X_valid, Y_valid), \n\u001b[1;32m     62\u001b[0m                         verbose = 1, callbacks = [ra_val, check_point, early_stop, TON])\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    315\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    142\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 143\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2505\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2507\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2508\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 2493\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   2494\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/Applications/Anaconda/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 137\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: Capsule"
     ]
    }
   ],
   "source": [
    "model = build_model(lr = 1e-3)\n",
    "pred = model.predict(test, batch_size = 512, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "With caps 1 bigru & fastext :\n",
    "Epoch 1/4\n",
    "43680/143613 [========>.....................] - ETA: 22:54 - loss: 0.0637 - acc: 0.9780\n",
    "                    \n",
    "143584/143613 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9809\n",
    "ROC-AUC - epoch: 1 - score: 0.987818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-9641b31fab80>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-9641b31fab80>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    With caps 1 bigru :\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "With caps 1 bigru :\n",
    "Epoch 1/4\n",
    " 43232/143613 [========>.....................] - ETA: 37:59 - loss: 0.0662 - acc: 0.9771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "With caps 2 different bi gru:\n",
    "  8256/143613 [>.............................] - ETA: 1:12:08 - loss: 0.0892 - acc: 0.9714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Without caps :\n",
    "Epoch 1/4\n",
    " 31648/143613 [=====>........................] - ETA: 36:26 - loss: 0.0653 - acc: 0.9780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "With caps :\n",
    "Epoch 1/4\n",
    " 17984/143613 [==>...........................] - ETA: 1:10:20 - loss: 0.0717 - acc: 0.9758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 143613 samples, validate on 15958 samples\n",
    "Epoch 1/4\n",
    "143584/143613 [============================>.] - ETA: 1s - loss: 0.0520 - acc: 0.9810\n",
    "ROC-AUC - epoch: 1 - score: 0.987583\n",
    "\n",
    "ROC-AUC of class toxic- epoch: 1 - score: 0.982831\n",
    "\n",
    "ROC-AUC of class severe_toxic- epoch: 1 - score: 0.986617\n",
    "\n",
    "ROC-AUC of class obscene- epoch: 1 - score: 0.990645\n",
    "\n",
    "ROC-AUC of class threat- epoch: 1 - score: 0.992318\n",
    "\n",
    "ROC-AUC of class insult- epoch: 1 - score: 0.987175\n",
    "\n",
    "ROC-AUC of class identity_hate- epoch: 1 - score: 0.985910\n",
    "\n",
    "Epoch 00001: val_loss improved from inf to 0.04189, saving model to best_model.hdf5\n",
    "143613/143613 [==============================] - 6860s 48ms/step - loss: 0.0520 - acc: 0.9810 - val_loss: 0.0419 - val_acc: 0.9836\n",
    "Epoch 2/4\n",
    "143584/143613 [============================>.] - ETA: 1s - loss: 0.0438 - acc: 0.9833\n",
    "ROC-AUC - epoch: 2 - score: 0.989061\n",
    "\n",
    "ROC-AUC of class toxic- epoch: 2 - score: 0.984466\n",
    "\n",
    "ROC-AUC of class severe_toxic- epoch: 2 - score: 0.987612\n",
    "\n",
    "ROC-AUC of class obscene- epoch: 2 - score: 0.991312\n",
    "\n",
    "ROC-AUC of class threat- epoch: 2 - score: 0.995831\n",
    "\n",
    "ROC-AUC of class insult- epoch: 2 - score: 0.988579\n",
    "\n",
    "ROC-AUC of class identity_hate- epoch: 2 - score: 0.986566\n",
    "\n",
    "Epoch 00002: val_loss did not improve\n",
    "143613/143613 [==============================] - 6816s 47ms/step - loss: 0.0438 - acc: 0.9833 - val_loss: 0.0460 - val_acc: 0.9825\n",
    "Epoch 3/4\n",
    " 35104/143613 [======>.......................] - ETA: 1:19:43 - loss: 0.0407 - acc: 0.9846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Last with num_vars\n",
    "\n",
    "Epoch 1/10\n",
    "12128/159571 [=>............................] - ETA: 1:47:40 - loss: 0.0738 - acc: 0.9759\n",
    "\n",
    "Epoch 1/10\n",
    "21760/159571 [===>..........................] - ETA: 1:40:56 - loss: 0.0657 - acc: 0.9776\n",
    "\n",
    "Epoch 1/10\n",
    "37088/159571 [=====>........................] - ETA: 1:29:43 - loss: 0.0601 - acc: 0.9790\n",
    "\n",
    "Epoch 1/10\n",
    "64896/159571  [===========>..................] - ETA: 1:09:30 - loss: 0.0555 - acc: 0.9801\n",
    "\n",
    "Epoch 1/10\n",
    "103264/159571 [==================>...........] - ETA: 41:44 - loss: 0.0522 - acc: 0.9809\n",
    "\n",
    "Epoch 1/10\n",
    "138144/159571 [========================>.....] - ETA: 16:41 - loss: 0.0507 - acc: 0.9813\n",
    "\n",
    "Epoch 1/10\n",
    "159571/159571 [==============================] - 7443s 47ms/step - loss: 0.0500 - acc: 0.9815\n",
    "        \n",
    "Epoch 2/10\n",
    " 22048/159571 [===>..........................] - ETA: 1:45:59 - loss: 0.0416 - acc: 0.9836\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "With caps, multiple dense:\n",
    "    \n",
    "Epoch 1/10\n",
    " 18944/159571 [==>...........................] - ETA: 3:03:43 - loss: 0.0766 - acc: 0.9730\n",
    "                        \n",
    "Epoch 1/10\n",
    " 29184/159571 [====>.........................] - ETA: 4:14:37 - loss: 0.0697 - acc: 0.9757\n",
    "                        \n",
    "Epoch 1/10\n",
    " 40192/159571 [======>.......................] - ETA: 3:58:25 - loss: 0.0660 - acc: 0.9767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77cf98ba3375982e99a91efce120aff9998a7750",
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Without caps\n",
    "\n",
    "Epoch 1/10\n",
    "  8576/159571 [>.............................] - ETA: 2:00:27 - loss: 0.1634 - acc: 0.9488\n",
    "                        \n",
    "Epoch 1/10\n",
    " 49280/159571 [========>.....................] - ETA: 2:03:26 - loss: 0.0833 - acc: 0.9726\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission[list_classes] = (pred)\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
