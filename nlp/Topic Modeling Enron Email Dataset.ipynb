{"cells":[{"metadata":{"_cell_guid":"df120470-b6a6-d1c5-2ecc-b7129b2466dd","trusted":true},"cell_type":"code","source":"import os, sys, email,re\nimport numpy as np \nimport pandas as pd\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set_style('whitegrid')\nimport wordcloud\n\n# Network analysis\nimport networkx as nx\n\n# NLP\nfrom nltk.tokenize.regexp import RegexpTokenizer\n\nfrom subprocess import check_output\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport gensim\nfrom gensim import corpora\nfrom nltk.corpus import stopwords \nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nfrom nltk.stem.porter import PorterStemmer","execution_count":51,"outputs":[]},{"metadata":{"_cell_guid":"2a6492f3-ffbc-93d2-1135-1f3e23088f8a","trusted":true},"cell_type":"code","source":"# Read the data into a DataFrame\nemails_df = pd.read_csv('/kaggle/input/emails.csv')\nprint(emails_df.shape)\nemails_df.head()","execution_count":52,"outputs":[{"output_type":"stream","text":"(517401, 2)\n","name":"stdout"},{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"                       file                                            message\n0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>allen-p/_sent_mail/1.</td>\n      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>allen-p/_sent_mail/10.</td>\n      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>allen-p/_sent_mail/100.</td>\n      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>allen-p/_sent_mail/1000.</td>\n      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>allen-p/_sent_mail/1001.</td>\n      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_cell_guid":"c55ba68d-6955-0d73-3aca-9f33bfc870ab","trusted":true},"cell_type":"code","source":"## Helper functions\ndef get_text_from_email(msg):\n    '''To get the content from email objects'''\n    parts = []\n    for part in msg.walk():\n        if part.get_content_type() == 'text/plain':\n            parts.append( part.get_payload() )\n    return ''.join(parts)\n\ndef split_email_addresses(line):\n    '''To separate multiple email addresses'''\n    if line:\n        addrs = line.split(',')\n        addrs = frozenset(map(lambda x: x.strip(), addrs))\n    else:\n        addrs = None\n    return addrs","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"4d7327aa-adfb-aee3-d4f1-e6631a7ddc9e","trusted":true},"cell_type":"code","source":"# Parse the emails into a list email objects\nmessages = list(map(email.message_from_string, emails_df['message']))\nemails_df.drop('message', axis=1, inplace=True)\n# Get fields from parsed email objects\nkeys = messages[0].keys()\nfor key in keys:\n    emails_df[key] = [doc[key] for doc in messages]\n# Parse content from emails\nemails_df['content'] = list(map(get_text_from_email, messages))\n# Split multiple email addresses\nemails_df['From'] = emails_df['From'].map(split_email_addresses)\nemails_df['To'] = emails_df['To'].map(split_email_addresses)\n\n# Extract the root of 'file' as 'user'\nemails_df['user'] = emails_df['file'].map(lambda x:x.split('/')[0])\ndel messages\n\nemails_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d641bec-4605-75c9-da9e-3d8cc4a64dfa","trusted":true},"cell_type":"code","source":"# Set index and drop columns with two few values\nemails_df = emails_df.set_index('Message-ID')\\\n    .drop(['file', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding'], axis=1)\n# Parse datetime\nemails_df['Date'] = pd.to_datetime(emails_df['Date'], infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6e42e883-9417-a7db-c77e-9b59e78b3c52","trusted":true},"cell_type":"code","source":"def clean(text):\n    stop = set(stopwords.words('english'))\n    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"))\n    exclude = set(string.punctuation) \n    lemma = WordNetLemmatizer()\n    porter= PorterStemmer()\n    \n    text=text.rstrip()\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    #stem = \" \".join(porter.stem(token) for token in normalized.split())\n    \n    return normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.sub(r'[^a-zA-Z]', ' ', \"etienne ,//hgjhk jgjhg\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5fc9545f-2207-8d5e-8380-58997549c84b","trusted":true},"cell_type":"code","source":"analysis_df=emails_df[['From', 'To', 'Date','content']].dropna().copy()\nanalysis_df = analysis_df.loc[analysis_df['To'].map(len) == 1]\nsub_df=analysis_df.sample(1000)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b62b77a3-fc79-b883-8eb3-67c4a77baed8","trusted":true},"cell_type":"code","source":"#sub_df[\"content\"]=sub_df[\"content\"].map(clean)\ntext_clean=[]\nfor text in sub_df['content']:\n    text_clean.append(clean(text).split())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89ab4b9a-9132-6fea-17b7-135f1dc00a7c","trusted":true},"cell_type":"code","source":"dictionary = corpora.Dictionary(text_clean)\ntext_term_matrix = [dictionary.doc2bow(text) for text in text_clean]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16714ab8-100b-93da-5ef9-1991e59a0668","trusted":true},"cell_type":"code","source":"Lda = gensim.models.ldamodel.LdaModel\nldamodel = Lda(text_term_matrix, num_topics=4, id2word = dictionary, passes=30)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a22fdc9-7bc1-aabe-686e-3c01d638978d","trusted":true},"cell_type":"code","source":"eng_stopwords = set(stopwords.words('english'))\ndef clean_text(text):\n    #text = BeautifulSoup(text, 'html.parser').get_text()\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    words = text.lower().split()\n    words = [w for w in words if w not in eng_stopwords]\n    return ' '.join(words)\n\nanalysis_df[\"clean_content\"]=analysis_df.content.apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2a311d8-f00a-f085-633b-dc0a64ad32d1","trusted":true},"cell_type":"code","source":"wordvector = TfidfVectorizer(analyzer='word', stop_words='english', max_df=0.4, min_df=5)\nshort_analysis=analysis_df.sample(5000)\nwordvector_fit = wordvector.fit_transform(short_analysis.clean_content)\nfeature = wordvector.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43ee9d65-133c-c4ba-f6cf-37f1c4571ca6","trusted":true},"cell_type":"code","source":"N = 4\nclf = KMeans(n_clusters=N, \n            max_iter=50, \n            init='k-means++', \n            n_init=1)\nlabels = clf.fit_predict(wordvector_fit)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce515e80-b3a2-183a-f20b-ab3c4360c015","trusted":true},"cell_type":"code","source":"wordvector_fit_2d = wordvector_fit.todense()\npca = PCA(n_components=2).fit(wordvector_fit_2d)\ndatapoint = pca.transform(wordvector_fit_2d)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eabec3f2-37aa-6a05-cf33-5aef39126c16","trusted":true},"cell_type":"code","source":"label = [\"#e05f14\", \"#e0dc14\", \"#2fe014\", \"#14d2e0\"]\ncolor = [label[i] for i in labels]\nplt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n\ncentroids = clf.cluster_centers_\ncentroidpoint = pca.transform(centroids)\nplt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}