{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import numpy as np, pandas as pd, random\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "from keras.layers import K, Activation, Average, Maximum\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,GlobalMaxPooling2D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TerminateOnNaN\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir=\"data/\"\n",
    "train = pd.read_csv(data_dir+\"train.csv\")\n",
    "test = pd.read_csv(data_dir+\"test.csv\")\n",
    "submission = pd.read_csv(data_dir+\"sample_submission.csv\")\n",
    "\n",
    "#embedding_path = data_dir+\"fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "embedding_path = data_dir+\"glove840b300dtxt/glove.840B.300d.txt\"\n",
    "\n",
    "max_features = 30000\n",
    "max_len = 150\n",
    "embed_size = 300\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "X_train['text'] = sequence.pad_sequences(list_tokenized_train, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test['text'] = sequence.pad_sequences(list_tokenized_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "train['num_words'] = train.comment_text.str.count('\\S+')\n",
    "test['num_words'] = test.comment_text.str.count('\\S+')\n",
    "train['num_comas'] = train.comment_text.str.count('\\.')\n",
    "test['num_comas'] = test.comment_text.str.count('\\.')\n",
    "train['num_bangs'] = train.comment_text.str.count('\\!')\n",
    "test['num_bangs'] = test.comment_text.str.count('\\!')\n",
    "train['num_quotas'] = train.comment_text.str.count('\\\"')\n",
    "test['num_quotas'] = test.comment_text.str.count('\\\"')\n",
    "train['avg_word'] = train.comment_text.str.len() / (1 + train.num_words)\n",
    "test['avg_word'] = test.comment_text.str.len() / (1 + test.num_words)\n",
    "scaler = MinMaxScaler()\n",
    "X_train['num_vars'] = scaler.fit_transform(train[['num_words','num_comas','num_bangs','num_quotas','avg_word']])\n",
    "X_test['num_vars'] = scaler.transform(test[['num_words','num_comas','num_bangs','num_quotas','avg_word']])\n",
    "\n",
    "N = len(train)\n",
    "indexs_val=[]\n",
    "for _ in range(N//10):\n",
    "    indexs_val.append(random.randint(0,N-1))\n",
    "indexs_val=list(set(indexs_val))\n",
    "indexs_train = [i for i in range(N) if i not in indexs_val]\n",
    "\n",
    "X_valid={}\n",
    "X_valid['text']=X_train['text'][indexs_val]\n",
    "X_valid['num_vars']=X_train['num_vars'][indexs_val]\n",
    "Y_valid=y[indexs_val]\n",
    "X_train['text']=X_train['text'][indexs_train]\n",
    "X_train['num_vars']=X_train['num_vars'][indexs_train]\n",
    "Y_train=y[indexs_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9055de15dd59ed4b0c5f5da3922775121cb080aa",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc55955b4eb1c36f7b4338a6d75215158280d1ec",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\nROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "            for i in range(len(list_classes)):\n",
    "                score2 = roc_auc_score(self.y_val[:,i], y_pred[:,i])\n",
    "                print(\"ROC-AUC of class {}- epoch: {:d} - score: {:.6f}\".format(list_classes[i],epoch+1, score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec39caf3396c6ea2da959c355ee62a0c177a34d9",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = \"best_model.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,save_best_only = True, mode = \"min\")\n",
    "ra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 2)\n",
    "lr_scheduler = LearningRateScheduler(schedule=lambda epoch_n: self.init_lr / (2**(epoch_n)), verbose = 1)\n",
    "TON = TerminateOnNaN()\n",
    "\n",
    "Routings = 5\n",
    "Num_capsule = 8\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 32\n",
    "\n",
    "def build_model(lr = 0.0):\n",
    "    \n",
    "    # Input\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    inp = Input(shape=(max_len, ), name=\"text\")\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    # CNN2D\n",
    "    y = Reshape((max_len, embed_size, 1))(x)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size), kernel_initializer='normal',activation='elu')(y)\n",
    "    maxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1))(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1))(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1))(conv_2)\n",
    "    maxpool_3 = MaxPool2D(pool_size=(max_len - filter_sizes[3] + 1, 1))(conv_3)\n",
    "    y = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    y = Flatten()(y)\n",
    "    \n",
    "    # Bigru\n",
    "    bigru = Bidirectional(GRU(128, activation='relu', dropout=dropout_p,recurrent_dropout=dropout_p, return_sequences=True))(x)\n",
    "    \n",
    "    # CNN 1D\n",
    "    conv1D = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(bigru)\n",
    "    avg_pool = GlobalAveragePooling1D()(conv1D)\n",
    "    max_pool = GlobalMaxPooling1D()(conv1D)\n",
    "    z = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    # Capsule\n",
    "    c = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True)(bigru)\n",
    "    c = Flatten()(c)\n",
    "    \n",
    "    # num_vars\n",
    "    n = Dense(16, activation = \"relu\", kernel_constraint=maxnorm(3))(num_vars)\n",
    "    \n",
    "    # output\n",
    "    a = Average()([y,z,c])\n",
    "    m = Maximum()([y,z,c])\n",
    "    out = concatenate([a,m,n])\n",
    "    out = Dense(6, activation = \"sigmoid\")(out)\n",
    "    model = Model(inputs=[inp,num_vars], outputs=out)\n",
    "    \n",
    "    # model\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, clipvalue=0.2), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, Y_train, batch_size = 32, epochs = 4, validation_data = ([X_valid['text'],X_valid['num_vars']], Y_valid), \n",
    "                        verbose = 1, callbacks = [ra_val, check_point, early_stop, TON])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model(lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test, batch_size = 512, verbose = 1)\n",
    "submission[list_classes] = (pred)\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
